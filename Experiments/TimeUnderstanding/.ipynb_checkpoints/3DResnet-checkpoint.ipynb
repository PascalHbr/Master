{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15f9b72",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3bd305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/phuber/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Choose the `slow_r50` model \n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31fa6f4",
   "metadata": {},
   "source": [
    "### Import remaining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2c82bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60270310",
   "metadata": {},
   "source": [
    "### Setup device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d41ac93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to GPU or CPU\n",
    "device = \"cpu\"\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69f19e2",
   "metadata": {},
   "source": [
    "### Download the id to label mapping for the Kinetics 400 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b7cd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
    "json_filename = \"kinetics_classnames.json\"\n",
    "try: urllib.URLopener().retrieve(json_url, json_filename)\n",
    "except: urllib.request.urlretrieve(json_url, json_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f470852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_filename, \"r\") as f:\n",
    "    kinetics_classnames = json.load(f)\n",
    "\n",
    "# Create an id to label name mapping\n",
    "kinetics_id_to_classname = {}\n",
    "for k, v in kinetics_classnames.items():\n",
    "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6b5704",
   "metadata": {},
   "source": [
    "### Define input transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eac59250",
   "metadata": {},
   "outputs": [],
   "source": [
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 8\n",
    "sampling_rate = 8\n",
    "frames_per_second = 30\n",
    "\n",
    "# Note that this transform is specific to the slow_R50 model.\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size=(crop_size, crop_size))\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The duration of the input clip is also specific to the model.\n",
    "clip_duration = (num_frames * sampling_rate)/frames_per_second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ee465b",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e042ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_link = \"https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\"\n",
    "video_path = 'archery.mp4'\n",
    "try: urllib.URLopener().retrieve(url_link, video_path)\n",
    "except: urllib.request.urlretrieve(url_link, video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2cc2cb",
   "metadata": {},
   "source": [
    "### Load the video and transform it to the input format required by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f62e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([3, 90, 240, 320])\n",
      "torch.Size([3, 8, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Select the duration of the clip to load by specifying the start and end duration\n",
    "# The start_sec should correspond to where the action occurs in the video\n",
    "start_sec = 0\n",
    "end_sec = start_sec + clip_duration\n",
    "end_sec = 3\n",
    "print(end_sec)\n",
    "\n",
    "# Initialize an EncodedVideo helper class and load the video\n",
    "video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "# Load the desired clip\n",
    "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "print(video_data['video'].shape)\n",
    "\n",
    "# Apply a transform to normalize the video input\n",
    "video_data = transform(video_data)\n",
    "print(video_data['video'].shape)\n",
    "\n",
    "# Move the inputs to the desired device\n",
    "inputs = video_data[\"video\"]\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e0feb",
   "metadata": {},
   "source": [
    "### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c84b8a45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 256, 256])\n",
      "tensor([[ 7.3658e+00,  4.5376e+00,  9.3245e-01, -1.5395e+00, -1.0053e+01,\n",
      "          6.9937e+01, -4.3952e+00,  2.3782e+00,  1.0018e+00,  1.5337e+00,\n",
      "         -4.3518e+00, -3.4403e+00,  7.3954e-01, -1.1670e+00,  1.1805e+00,\n",
      "         -1.4303e-02, -1.0332e+00,  9.5922e-01, -1.4973e+00, -3.0453e+00,\n",
      "          1.1070e+00,  5.4270e+00, -1.5748e-01, -3.6760e+00,  8.3234e+00,\n",
      "         -2.7329e+00, -3.2130e+00, -6.4531e+00, -2.6370e+00, -5.2812e+00,\n",
      "         -3.8525e+00, -2.5389e+00, -3.4044e+00, -6.7294e+00, -1.4267e+00,\n",
      "          8.0389e-01, -7.1033e+00, -4.8424e+00, -1.7084e-01,  3.4026e+00,\n",
      "          8.4727e+00,  4.3644e+00,  6.1376e+00,  2.9348e+00, -1.5195e+00,\n",
      "         -4.1009e-01, -2.4971e+00,  4.7193e+00,  1.3953e-01,  5.1990e+00,\n",
      "          1.1383e+00,  2.0798e+00, -3.9347e+00,  4.4492e+00,  5.8390e-01,\n",
      "          6.1401e+00,  8.1690e+00, -5.0845e+00, -6.2952e+00, -3.2534e+00,\n",
      "         -6.2249e+00,  4.6930e+00, -1.8951e+00, -7.9065e+00, -8.9854e+00,\n",
      "          2.4727e+00, -1.8113e+00,  4.6379e+00,  2.0248e+00,  4.3954e+00,\n",
      "         -5.6602e+00, -7.1009e-01,  5.1525e+00, -1.4079e+00, -6.6181e-02,\n",
      "          2.4574e+00, -7.0366e-01, -8.9052e+00,  2.0710e-03, -5.4489e+00,\n",
      "          1.0243e+00, -5.2677e+00, -2.4072e+00,  5.7187e+00, -4.5517e+00,\n",
      "         -8.7781e+00, -4.2549e+00, -8.8772e-01, -4.9199e-01,  4.3144e-02,\n",
      "          8.4289e+00, -4.4571e+00,  8.8162e+00, -2.1915e+00,  2.3555e+00,\n",
      "         -2.4470e+00, -4.9254e+00, -1.1111e+01, -2.8455e-01, -1.1172e+01,\n",
      "          5.9547e+00,  7.9407e+00,  4.1011e+00, -5.7628e-01,  8.8820e+00,\n",
      "         -1.2815e+00, -1.4591e+00, -2.5981e+00, -7.1504e+00, -1.0102e+01,\n",
      "         -3.9247e+00,  6.0758e-02, -6.9426e+00, -5.3497e+00, -1.9471e+00,\n",
      "         -5.6464e+00, -7.3684e+00, -1.3830e+00, -5.6268e+00,  7.6227e+00,\n",
      "         -1.6392e+00,  3.2746e+00,  1.8931e+00,  3.4667e-01, -5.2142e+00,\n",
      "         -3.5304e+00, -2.8392e+00, -8.6207e+00, -2.9540e+00, -6.0042e+00,\n",
      "          5.8029e+00, -5.5343e+00, -1.0974e+01, -7.5732e+00,  7.5400e+00,\n",
      "         -6.1789e+00, -1.9426e+00,  1.3736e+00, -2.6955e+00, -6.6940e-01,\n",
      "          3.2815e+00,  1.0446e+01,  1.2275e+01,  5.9181e+00, -7.8862e+00,\n",
      "         -3.9728e+00,  5.3367e+00,  1.9641e+00, -2.8523e-01, -1.7402e+00,\n",
      "          1.7580e+00,  2.7826e+00,  4.5756e+00,  7.3188e+00, -2.7510e+00,\n",
      "          1.4155e+00, -5.1667e+00, -2.1101e+00,  3.2391e+00,  3.6164e+00,\n",
      "         -1.2533e+00,  1.0361e+01,  9.8735e+00,  5.4541e+00, -1.0919e+01,\n",
      "         -1.4261e+00,  1.2205e+01, -1.0750e+01,  1.1629e-01,  4.3439e-01,\n",
      "          4.3546e+00, -1.0043e+00, -3.1459e+00, -3.0573e+00,  4.2777e+00,\n",
      "          7.4195e-02, -7.5682e-01,  6.2616e+00, -5.9246e+00, -2.0428e-01,\n",
      "         -9.6632e-01, -4.1331e-01,  1.5780e+00, -3.5268e+00, -5.3090e+00,\n",
      "         -6.6732e+00, -1.0249e+01,  3.6232e+00, -5.5952e+00, -1.6924e+00,\n",
      "         -4.7783e+00, -4.7133e+00,  6.4852e+00, -3.9790e+00, -9.5904e+00,\n",
      "         -1.1442e+01, -2.3019e+00,  1.8601e+00, -6.5721e+00,  2.5593e+00,\n",
      "         -4.5780e+00,  5.4563e+00, -7.6789e-01,  4.5703e+00, -5.4568e+00,\n",
      "          1.1312e+01,  1.1543e+00, -5.7869e-01,  1.3870e+00,  3.1540e+00,\n",
      "         -8.6222e-02, -5.9816e+00,  4.5567e-01, -6.3978e+00,  2.6862e+00,\n",
      "          8.6328e+00, -4.1123e+00,  1.5862e+00,  8.8852e+00,  8.4214e+00,\n",
      "         -3.7637e+00,  1.4961e+00, -3.4488e+00,  6.3796e+00,  1.3960e+00,\n",
      "          8.5662e+00, -2.0360e+00,  8.6717e+00,  9.8473e-01,  5.6286e+00,\n",
      "          2.2310e-01,  8.8625e+00,  5.5222e+00,  5.7486e+00, -2.5526e+00,\n",
      "         -8.8936e-02,  1.6677e+00,  1.9812e-01, -2.2849e+00, -5.9777e+00,\n",
      "          1.6356e+01, -6.4220e+00,  1.4490e+00,  4.4964e+00,  2.5808e+00,\n",
      "          1.1855e+00,  8.1434e+00,  9.1367e+00,  2.7515e+00,  2.2598e+00,\n",
      "          5.6153e+00,  5.4483e+00,  1.3479e+00,  4.9861e+00,  4.1165e+00,\n",
      "          3.7252e+00, -5.9903e+00, -3.2563e+00, -7.8493e+00, -4.2653e+00,\n",
      "         -3.3848e+00, -7.1022e+00, -1.5352e+00, -1.2742e+00, -1.8865e+00,\n",
      "         -1.9394e+00,  4.6001e+00, -5.8129e-01, -2.9347e+00, -2.9635e+00,\n",
      "         -7.8349e+00,  9.5143e+00,  6.9994e+00,  1.2755e+01, -3.3673e+00,\n",
      "          1.9119e+00, -3.2414e+00,  2.9503e+00,  3.3438e+00,  2.0201e+00,\n",
      "          2.3841e+00, -7.6161e+00,  2.0751e+00, -8.6074e+00, -9.0140e+00,\n",
      "          1.0478e+00,  2.2297e+00, -5.2219e+00, -7.1090e+00, -1.4050e+00,\n",
      "          3.0030e+00,  8.3749e-01, -2.5506e-01, -9.6236e+00,  8.2650e+00,\n",
      "         -1.0115e+01, -5.4060e-01,  4.2380e+00,  7.1521e+00, -4.4045e+00,\n",
      "         -2.3217e+00, -5.3064e+00,  3.8123e+00, -2.7357e+00, -1.3327e+00,\n",
      "         -3.9749e+00,  4.7994e+00,  7.1377e+00,  5.2254e+00,  1.9752e+00,\n",
      "         -8.0879e-01,  7.7780e-01, -5.9166e-01,  1.2735e+01,  3.4201e-01,\n",
      "         -1.1582e+00,  1.1991e+00,  5.9995e+00, -4.2716e-01,  1.1137e-01,\n",
      "          2.0515e+00, -4.6788e+00, -5.7116e-01,  4.2286e+00, -3.0893e+00,\n",
      "          8.3591e-01,  1.6684e+00,  5.9302e-01,  2.5547e+00, -3.1254e-01,\n",
      "          3.1095e+00, -5.1298e-01, -2.3436e+00,  1.5193e+01,  1.0862e+00,\n",
      "          1.0136e+00, -7.5589e+00,  2.9940e-01, -3.0943e+00, -6.5889e-01,\n",
      "         -4.4119e+00, -1.6712e+00, -1.2050e+01, -1.0153e+00,  6.2989e+00,\n",
      "          9.0653e+00,  6.5620e+00, -4.3712e+00, -4.0630e+00, -5.5818e+00,\n",
      "          3.7110e+00, -6.9558e+00, -2.5936e-01, -6.8080e-01, -4.3633e+00,\n",
      "         -2.4111e-01,  2.2597e+01, -1.6409e-01,  2.6682e+00, -5.5219e+00,\n",
      "         -8.5306e+00,  5.5563e+00, -4.8830e+00,  3.4824e+00,  4.8197e+00,\n",
      "          3.1177e+00,  1.1288e+01, -6.8717e-01, -3.1661e+00,  6.0601e+00,\n",
      "         -3.7487e+00,  1.7879e+00,  2.4087e+00, -3.9833e+00,  7.7473e+00,\n",
      "          2.6513e+00, -6.7124e+00, -5.0098e+00, -2.9992e-01, -3.6038e+00,\n",
      "         -1.1306e+01, -7.3201e+00, -1.7744e+00, -3.9725e-01, -5.1775e+00,\n",
      "          5.0163e+00, -8.3645e+00, -4.7268e+00, -7.7200e+00, -5.6817e+00,\n",
      "          1.1032e+00,  4.3189e+00,  1.6693e+00,  1.7683e+00, -2.7700e+00,\n",
      "         -2.3636e+00, -6.6383e+00, -2.7059e-01,  6.6280e+00,  7.2546e-01]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 400])\n",
      "Top 5 predicted labels: archery, throwing axe, playing paintball, stretching arm, riding or walking with horse\n"
     ]
    }
   ],
   "source": [
    "# Pass the input clip through the model\n",
    "preds = model(inputs[None, ...])\n",
    "print(inputs.shape)\n",
    "print(preds)\n",
    "print(preds.shape)\n",
    "\n",
    "# Get the predicted classes\n",
    "post_act = torch.nn.Softmax(dim=1)\n",
    "preds = post_act(preds)\n",
    "pred_classes = preds.topk(k=5).indices[0]\n",
    "\n",
    "# Map the predicted classes to the label names\n",
    "pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
    "print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7be41461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c83e530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchvideo.models.head import ResNetBasicHead\n",
    "\n",
    "classification_head = ResNetBasicHead(pool=nn.AvgPool3d(kernel_size=(8, 7, 7), stride=(1, 1, 1), padding=(0, 0, 0)),\n",
    "                                     dropout=nn.Dropout(p=0.5, inplace=False),\n",
    "                                     proj=nn.Linear(in_features=2048, out_features=10, bias=True),\n",
    "                                     output_pool=nn.AdaptiveAvgPool3d(output_size=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e369b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.blocks._modules['5'] = classification_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3f16af09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.5.proj Linear(in_features=2048, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_modules():\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "        print(name, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd691c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
